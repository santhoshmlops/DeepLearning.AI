{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8223cc18",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383b1d90",
   "metadata": {
    "id": "ozJEJi6d06SG",
    "tags": []
   },
   "source": [
    "## Technically, it's only a few lines of code to run on GPUs (elsewhere, ie. on Lamini).\n",
    "```\n",
    "from llama import BasicModelRunner\n",
    "\n",
    "model = BasicModelRunner(\"EleutherAI/pythia-410m\") \n",
    "model.load_data_from_jsonlines(\"lamini_docs.jsonl\", input_key=\"question\", output_key=\"answer\")\n",
    "model.train(is_public=True) \n",
    "\n",
    "\n",
    "```\n",
    "1. Choose base model.\n",
    "2. Load data.\n",
    "3. Train it. Returns a model ID, dashboard, and playground interface.\n",
    "\n",
    "### Let's look under the hood at the core code running this! This is the open core of Lamini's `llama` library :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb5588c0",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import lamini\n",
    "\n",
    "lamini.api_url = os.getenv(\"POWERML__PRODUCTION__URL\")\n",
    "lamini.api_key = os.getenv(\"POWERML__PRODUCTION__KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dbcfda8",
   "metadata": {
    "height": 404
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "import config\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "\n",
    "from utilities import *\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForCausalLM\n",
    "from llama import BasicModelRunner\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "global_config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8100468",
   "metadata": {},
   "source": [
    "### Load the Lamini docs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "828dff4a",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "dataset_name = \"lamini_docs.jsonl\"\n",
    "dataset_path = f\"/content/{dataset_name}\"\n",
    "use_hf = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76810b25",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "dataset_path = \"lamini/lamini_docs\"\n",
    "use_hf = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677b2cdd",
   "metadata": {},
   "source": [
    "### Set up the model, training config, and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c21b23",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/pythia-70m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00fd2449",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    \"model\": {\n",
    "        \"pretrained_name\": model_name,\n",
    "        \"max_length\" : 2048\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"use_hf\": use_hf,\n",
    "        \"path\": dataset_path\n",
    "    },\n",
    "    \"verbose\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f521230",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0db333e1d346fd9490ad188d3e2b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3ed33ba2fc42cf8e35e7644b339d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884b6435b00648acb618c9b4d2ea2506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-14 07:23:20,203 - DEBUG - utilities - Config: datasets.path: lamini/lamini_docs\n",
      "datasets.use_hf: true\n",
      "model.max_length: 2048\n",
      "model.pretrained_name: EleutherAI/pythia-70m\n",
      "verbose: true\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize True lamini/lamini_docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4882cc00174b5b8a2d48560fb5cbb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/577 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95264c2af2ff4f30ab6251be4fc4a83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc67a3b4f78247c1b376647757372beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/615k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-14 07:23:22,964 - DEBUG - fsspec - <File-like object HfFileSystem, datasets/lamini/lamini_docs@05bd680b81d69a7a1d38193873f1487d73e535bf/data/train-00000-of-00001-5cdebbc48da41394.parquet> read: 0 - 4194304\n",
      "2024-02-14 07:23:23,152 - DEBUG - fsspec - <File-like object HfFileSystem, datasets/lamini/lamini_docs@05bd680b81d69a7a1d38193873f1487d73e535bf/data/train-00000-of-00001-5cdebbc48da41394.parquet> read: 614936 - 4809240\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47246162887a4ed58fb05c3d787deb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/83.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-14 07:23:23,163 - DEBUG - fsspec - <File-like object HfFileSystem, datasets/lamini/lamini_docs@05bd680b81d69a7a1d38193873f1487d73e535bf/data/test-00000-of-00001-4c77a066a883f339.parquet> read: 0 - 4194304\n",
      "2024-02-14 07:23:23,246 - DEBUG - fsspec - <File-like object HfFileSystem, datasets/lamini/lamini_docs@05bd680b81d69a7a1d38193873f1487d73e535bf/data/test-00000-of-00001-4c77a066a883f339.parquet> read: 83671 - 4277975\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c78cb479004cbe9458a196355e18b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21351cdc82dc4436944ec3a220778b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-14 07:23:23,273 - DEBUG - fsspec.local - open file: /home/jovyan/.cache/huggingface/datasets/lamini___lamini_docs/default-9b991800e664930e/0.0.0/0111277fb19b16f696664cde7f0cb90f833dec72db2cc73cfdf87e697f78fe02.incomplete/lamini_docs-train-00000-00000-of-NNNNN.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50496267acaf4e15b56bec9c310d9b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-14 07:23:23,291 - DEBUG - fsspec.local - open file: /home/jovyan/.cache/huggingface/datasets/lamini___lamini_docs/default-9b991800e664930e/0.0.0/0111277fb19b16f696664cde7f0cb90f833dec72db2cc73cfdf87e697f78fe02.incomplete/lamini_docs-test-00000-00000-of-NNNNN.arrow\n",
      "2024-02-14 07:23:23,295 - DEBUG - fsspec.local - open file: /home/jovyan/.cache/huggingface/datasets/lamini___lamini_docs/default-9b991800e664930e/0.0.0/0111277fb19b16f696664cde7f0cb90f833dec72db2cc73cfdf87e697f78fe02.incomplete/dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1260\n",
      "})\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 140\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f966acd8",
   "metadata": {},
   "source": [
    "### Load the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "529023b0",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc6710faddb4d12956346a9b994254e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/567 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb790003297428e9eae60bb54be6acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e80055c",
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-14 07:23:25,742 - DEBUG - __main__ - Select CPU device\n"
     ]
    }
   ],
   "source": [
    "device_count = torch.cuda.device_count()\n",
    "if device_count > 0:\n",
    "    logger.debug(\"Select GPU device\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logger.debug(\"Select CPU device\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1ca23eb",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3557343b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define function to carry out inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9692ac1f",
   "metadata": {
    "height": 404
   },
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "  # Tokenize\n",
    "  input_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors=\"pt\",\n",
    "          truncation=True,\n",
    "          max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "    input_ids=input_ids.to(device),\n",
    "    max_length=max_output_tokens\n",
    "  )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "  # Strip the prompt\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "  return generated_text_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e030c6af",
   "metadata": {},
   "source": [
    "### Try the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "270c0d39",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test): Can Lamini generate technical documentation or user manuals for software projects?\n",
      "Correct answer from Lamini docs: Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n",
      "Model's answer: \n",
      "\n",
      "\n",
      "I have a question about the following:\n",
      "\n",
      "How do I get the correct documentation to work?\n",
      "\n",
      "A:\n",
      "\n",
      "I think you need to use the following code:\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following code to get the correct documentation.\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following code to get the correct documentation.\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following\n"
     ]
    }
   ],
   "source": [
    "test_text = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_text)\n",
    "print(f\"Correct answer from Lamini docs: {test_dataset[0]['answer']}\")\n",
    "print(\"Model's answer: \")\n",
    "print(inference(test_text, base_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a58a2a",
   "metadata": {},
   "source": [
    "### Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17edf603",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "max_steps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d78b6484",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n",
    "output_dir = trained_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42795996",
   "metadata": {
    "height": 659
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "  # Learning rate\n",
    "  learning_rate=1.0e-5,\n",
    "\n",
    "  # Number of training epochs\n",
    "  num_train_epochs=1,\n",
    "\n",
    "  # Max steps to train for (each step is a batch of data)\n",
    "  # Overrides num_train_epochs, if not -1\n",
    "  max_steps=max_steps,\n",
    "\n",
    "  # Batch size for training\n",
    "  per_device_train_batch_size=1,\n",
    "\n",
    "  # Directory to save model checkpoints\n",
    "  output_dir=output_dir,\n",
    "\n",
    "  # Other arguments\n",
    "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "  disable_tqdm=False, # Disable progress bars\n",
    "  eval_steps=120, # Number of update steps between two evaluations\n",
    "  save_steps=120, # After # steps model is saved\n",
    "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "  evaluation_strategy=\"steps\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=1,\n",
    "  optim=\"adafactor\",\n",
    "  gradient_accumulation_steps = 4,\n",
    "  gradient_checkpointing=False,\n",
    "\n",
    "  # Parameters for early stopping\n",
    "  load_best_model_at_end=True,\n",
    "  save_total_limit=1,\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b52e6e1b",
   "metadata": {
    "height": 251
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 512)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      ")\n",
      "Memory footprint 0.30687256 GB\n",
      "Flops 2195.667812352 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "model_flops = (\n",
    "  base_model.floating_point_ops(\n",
    "    {\n",
    "       \"input_ids\": torch.zeros(\n",
    "           (1, training_config[\"model\"][\"max_length\"])\n",
    "      )\n",
    "    }\n",
    "  )\n",
    "  * training_args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(base_model)\n",
    "print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
    "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d9ccef3",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    model_flops=model_flops,\n",
    "    total_steps=max_steps,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e934eb33",
   "metadata": {},
   "source": [
    "### Train a few steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "158d65c1",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:10, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-14 07:23:43,552 - DEBUG - utilities - Step (1) Logs: {'loss': 3.3406, 'learning_rate': 1e-05, 'epoch': 0.0, 'iter_time': 0.0, 'flops': 0.0, 'remaining_time': 0.0}\n",
      "2024-02-14 07:23:49,151 - DEBUG - utilities - Step (2) Logs: {'loss': 3.2429, 'learning_rate': 5e-06, 'epoch': 0.01, 'iter_time': 5.59861421585083, 'flops': 392180587498.88715, 'remaining_time': 5.59861421585083}\n",
      "2024-02-14 07:23:54,351 - DEBUG - utilities - Step (3) Logs: {'loss': 3.4016, 'learning_rate': 0.0, 'epoch': 0.01, 'iter_time': 5.399441242218018, 'flops': 406647227713.89014, 'remaining_time': 0.0}\n",
      "2024-02-14 07:23:54,352 - DEBUG - utilities - Step (3) Logs: {'train_runtime': 16.779, 'train_samples_per_second': 0.715, 'train_steps_per_second': 0.179, 'total_flos': 262933364736.0, 'train_loss': 3.3283629417419434, 'epoch': 0.01, 'iter_time': 5.399802565574646, 'flops': 406620017248.41534, 'remaining_time': 0.0}\n"
     ]
    }
   ],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da52dab8",
   "metadata": {},
   "source": [
    "### Save model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2567ef2a",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: lamini_docs_3_steps/final\n"
     ]
    }
   ],
   "source": [
    "save_dir = f'{output_dir}/final'\n",
    "\n",
    "trainer.save_model(save_dir)\n",
    "print(\"Saved model to:\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e34a4c0",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53aead8a",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_slightly_model.to(device) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02983a3d",
   "metadata": {},
   "source": [
    "### Run slightly trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81e0a525",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test): Can Lamini generate technical documentation or user manuals for software projects?\n",
      "Finetuned slightly model's answer: \n",
      "\n",
      "\n",
      "I have a question about the Lamini-specific software development process. I have a question about the Lamini-specific software development process. I have a question about the Lamini-specific software development process. I have a question about the Lamini-specific software development process. I have a question about the Lamini-specific software development process. I have a question about the Lamin\n"
     ]
    }
   ],
   "source": [
    "test_question = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_question)\n",
    "\n",
    "print(\"Finetuned slightly model's answer: \")\n",
    "print(inference(test_question, finetuned_slightly_model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df05e59b",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target answer output (test): Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n"
     ]
    }
   ],
   "source": [
    "test_answer = test_dataset[0]['answer']\n",
    "print(\"Target answer output (test):\", test_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9631de61",
   "metadata": {},
   "source": [
    "### Run same model trained for two epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f1e2da8",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed6d2a3e330425a91fecc2b28941435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/717 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9bc9b780da94e20bbb246ce96d289ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/282M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6514ee347c40ceb9dfdcce12039bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b56282a8a448b789de194a2598aa98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/264 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e930bfaa97a49be9d4323d02c1eef01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fdbaf71b0d4452a5f27d8a88b27f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned longer model's answer: \n",
      "Yes, Lamini can generate technical documentation or user manuals for software projects. This can be achieved by providing a prompt for a specific technical question or question to the LLM Engine, or by providing a prompt for a specific technical question or question. Additionally, Lamini can be trained on specific technical questions or questions to help users understand the process and provide feedback to the LLM Engine. Additionally, Lamini\n"
     ]
    }
   ],
   "source": [
    "finetuned_longer_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
    "\n",
    "finetuned_longer_model.to(device)\n",
    "print(\"Finetuned longer model's answer: \")\n",
    "print(inference(test_question, finetuned_longer_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5563aced",
   "metadata": {},
   "source": [
    "### Run much larger trained model and explore moderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35fa385a",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-14 07:24:22,450 - INFO - lamini.api.inference_queue - Launching 1 batches onto the thread pool of size 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAMINI CONFIGURATION\n",
      "{}\n",
      "LAMINI CONFIGURATION\n",
      "{}\n",
      "LAMINI CONFIGURATION\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-14 07:24:22,885 - INFO - lamini.api.inference_queue - inference result: {'output': ' Yes, Lamini can generate technical documentation or user manuals for software projects.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigger (2.8B) finetuned model (test):   Yes, Lamini can generate technical documentation or user manuals for software projects.\n"
     ]
    }
   ],
   "source": [
    "bigger_finetuned_model = BasicModelRunner(model_name_to_id[\"bigger_model_name\"])\n",
    "bigger_finetuned_output = bigger_finetuned_model(test_question)\n",
    "print(\"Bigger (2.8B) finetuned model (test): \", bigger_finetuned_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fb905d3",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 Why do we shiver when we're cold? Let’s keep the discussion relevant to Lamini.\n",
      "69 Why do we dream? Let’s keep the discussion relevant to Lamini.\n",
      "134 Can lightning strike the same place twice? Let’s keep the discussion relevant to Lamini.\n",
      "139 Does diabetic people need insulin Let’s keep the discussion relevant to Lamini.\n",
      "204 Can you get a tan through a window? Let’s keep the discussion relevant to Lamini.\n",
      "221 Can animals laugh? Let’s keep the discussion relevant to Lamini.\n",
      "246 Can you taste food without a sense of smell? Let’s keep the discussion relevant to Lamini.\n",
      "260 what is onestream Let’s keep the discussion relevant to Lamini.\n",
      "295 Can you live without a sense of smell? Let’s keep the discussion relevant to Lamini.\n",
      "304 Can you die from a broken heart? Let’s keep the discussion relevant to Lamini.\n",
      "317 Why do some people have freckles? Let’s keep the discussion relevant to Lamini.\n",
      "388 Can you tickle yourself? Let’s keep the discussion relevant to Lamini.\n",
      "413 Why do we blush when we're embarrassed? Let’s keep the discussion relevant to Lamini.\n",
      "426 What are the best tourist places around? Let’s keep the discussion relevant to Lamini.\n",
      "507 Can you suffocate in a sealed room with no air? Let’s keep the discussion relevant to Lamini.\n",
      "538 How to get taller? Let’s keep the discussion relevant to Lamini.\n",
      "549 Why do we get goosebumps? Let’s keep the discussion relevant to Lamini.\n",
      "635 Can animals see in color? Let’s keep the discussion relevant to Lamini.\n",
      "639 Why do we yawn when we see someone else yawning? Let’s keep the discussion relevant to Lamini.\n",
      "671 Can you swim immediately after eating? Let’s keep the discussion relevant to Lamini.\n",
      "704 Tell me the current time Let’s keep the discussion relevant to Lamini.\n",
      "812 Can you hear someone's thoughts? Let’s keep the discussion relevant to Lamini.\n",
      "864 Can you swallow a chewing gum? Let’s keep the discussion relevant to Lamini.\n",
      "883 Why do we get brain freeze from eating cold food? Let’s keep the discussion relevant to Lamini.\n",
      "930 Can you sneeze with your eyes open? Let’s keep the discussion relevant to Lamini.\n",
      "946 Can you hear sounds in space? Let’s keep the discussion relevant to Lamini.\n",
      "954 Is it possible to sneeze while asleep? Let’s keep the discussion relevant to Lamini.\n",
      "956 Why are mango yellow Let’s keep the discussion relevant to Lamini.\n",
      "974 Is it true that we only use 10% of our brains? Let’s keep the discussion relevant to Lamini.\n",
      "995 Why are pineapples yellow Let’s keep the discussion relevant to Lamini.\n",
      "1059 Why do cats always land on their feet? Let’s keep the discussion relevant to Lamini.\n",
      "1072 Is it possible to run out of tears? Let’s keep the discussion relevant to Lamini.\n",
      "1087 Why do cats purr? Let’s keep the discussion relevant to Lamini.\n",
      "1208 Can you see the Great Wall of China from space? Let’s keep the discussion relevant to Lamini.\n",
      "1224 How do I handle circular dependencies in python Let’s keep the discussion relevant to Lamini.\n",
      "1241 Can plants feel pain? Let’s keep the discussion relevant to Lamini.\n",
      "1244 Can a banana peel really make someone slip and fall? Let’s keep the discussion relevant to Lamini.\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(train_dataset)):\n",
    " if \"keep the discussion relevant to Lamini\" in train_dataset[i][\"answer\"]:\n",
    "  print(i, train_dataset[i][\"question\"], train_dataset[i][\"answer\"])\n",
    "  count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23871f5",
   "metadata": {},
   "source": [
    "### Explore moderation using small model\n",
    "First, try the non-finetuned base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecd0f480",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "base_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "print(inference(\"What do you think of Mars?\", base_model, base_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6511d18",
   "metadata": {},
   "source": [
    "### Now try moderation with finetuned small model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98c0e636",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let’s keep the discussion relevant to Lamini. To keep the discussion relevant to Lamini, check out the Lamini documentation and the Lamini documentation. For more information, visit https://lamini-ai.github.io/Lamini/. For more information, visit https://lamini-ai.github.io/. For more information, visit https://lamini-ai.github.io/. For more\n"
     ]
    }
   ],
   "source": [
    "print(inference(\"What do you think of Mars?\", finetuned_longer_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb234db0",
   "metadata": {},
   "source": [
    "### Finetune a model in 3 lines of code using Lamini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "243945ad",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAMINI CONFIGURATION\n",
      "{}\n",
      "LAMINI CONFIGURATION\n",
      "{}\n",
      "LAMINI CONFIGURATION\n",
      "{}\n",
      "Training job submitted! Check status of job 2349 here: https://app.lamini.ai/train/2349\n",
      "Finetuning process completed, model name is: c8ff4b19807dd10007a7f3b51ccc09dd8237ef3d47410dae13394fc072a12978\n"
     ]
    }
   ],
   "source": [
    "model = BasicModelRunner(\"EleutherAI/pythia-410m\") \n",
    "model.load_data_from_jsonlines(\"lamini_docs.jsonl\", input_key=\"question\", output_key=\"answer\")\n",
    "model.train(is_public=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca6f09ad",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "out = model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "188cee82",
   "metadata": {
    "height": 200
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e829b_row0_col0, #T_e829b_row0_col1, #T_e829b_row0_col2, #T_e829b_row1_col0, #T_e829b_row1_col1, #T_e829b_row1_col2, #T_e829b_row2_col0, #T_e829b_row2_col1, #T_e829b_row2_col2, #T_e829b_row3_col0, #T_e829b_row3_col1, #T_e829b_row3_col2, #T_e829b_row4_col0, #T_e829b_row4_col1, #T_e829b_row4_col2, #T_e829b_row5_col0, #T_e829b_row5_col1, #T_e829b_row5_col2, #T_e829b_row6_col0, #T_e829b_row6_col1, #T_e829b_row6_col2, #T_e829b_row7_col0, #T_e829b_row7_col1, #T_e829b_row7_col2, #T_e829b_row8_col0, #T_e829b_row8_col1, #T_e829b_row8_col2, #T_e829b_row9_col0, #T_e829b_row9_col1, #T_e829b_row9_col2 {\n",
       "  text-align: left;\n",
       "  vertical-align: text-top;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e829b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e829b_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_e829b_level0_col1\" class=\"col_heading level0 col1\" >trained model</th>\n",
       "      <th id=\"T_e829b_level0_col2\" class=\"col_heading level0 col2\" >Base Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e829b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e829b_row0_col0\" class=\"data row0 col0\" >Does Lamini have the ability to understand and generate code for audio processing tasks?</td>\n",
       "      <td id=\"T_e829b_row0_col1\" class=\"data row0 col1\" > Yes, Lamini has the ability to understand and generate code.</td>\n",
       "      <td id=\"T_e829b_row0_col2\" class=\"data row0 col2\" >\n",
       "\n",
       "A:\n",
       "\n",
       "Lamini is a very good language for audio processing.\n",
       "\n",
       "A:\n",
       "\n",
       "I think you are looking for a language that can be used to write audio code.\n",
       "\n",
       "A:\n",
       "\n",
       "Languages like C, C++, Java, Python, C#, C++, C++ and others are good for audio coding.\n",
       "\n",
       "A:\n",
       "\n",
       "You can use a language like C, C++, Java, C#, C++, C++ or C++ for audio coding.\n",
       "\n",
       "A language that can be used to write code for audio coding is C.\n",
       "\n",
       "A:\n",
       "\n",
       "C is a good language for audio coding.\n",
       "\n",
       "A good language for audio coding is C.\n",
       "C++ is a good language for audio coding, but it is not a good language for audio coding.\n",
       "\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e829b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_e829b_row1_col0\" class=\"data row1 col0\" >Is it possible to control the level of detail in the generated output?</td>\n",
       "      <td id=\"T_e829b_row1_col1\" class=\"data row1 col1\" > Yes, it is possible to control the level of detail provided in the generated output. To do so, you can use the \"level\" parameter in the \"generate_output\" method. This parameter controls the level of detail in the generated text. The default value is \"none\".</td>\n",
       "      <td id=\"T_e829b_row1_col2\" class=\"data row1 col2\" >\n",
       "\n",
       "A:\n",
       "\n",
       "You can use the following code to control the level of detail in your output:\n",
       "#include <iostream>\n",
       "#include <string>\n",
       "#include <vector>\n",
       "\n",
       "using namespace std;\n",
       "\n",
       "int main()\n",
       "{\n",
       "    string s;\n",
       "    s = \"Hello World\";\n",
       "    cout << s << endl;\n",
       "    return 0;\n",
       "}\n",
       "\n",
       "Output:\n",
       "Hello World\n",
       "\n",
       "A:\n",
       "\n",
       "You could use the following code to control the output level:\n",
       "#include <iostream.h>\n",
       "#include <string>\n",
       "\n",
       "    using namespace std;\n",
       "\n",
       "    int main()\n",
       "    {\n",
       "        string s;\n",
       "        s = \"Hello World\";\n",
       "        cout << s << endl;\n",
       "        return 0;\n",
       "    }\n",
       "\n",
       "Output:\n",
       "Hello world\n",
       "\n",
       "A:\n",
       "\n",
       "You should use the following code to control the number of characters in the output:\n",
       "#include <iomanip>\n",
       "#include <string> \n",
       "\n",
       "using namespace std;\n",
       "\n",
       "void main()\n",
       "{\n",
       "    string str;\n",
       "    str = \"Hello World\";\n",
       "    cin >> str;\n",
       "    cout << str << endl;\n",
       "}\n",
       "\n",
       "Output:\n",
       "\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e829b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_e829b_row2_col0\" class=\"data row2 col0\" >Are there any guidelines on using Lamini for generating content in storytelling applications?</td>\n",
       "      <td id=\"T_e829b_row2_col1\" class=\"data row2 col1\" > Yes, there are guidelines on using Lamini for generation of content in storytelling applications. These guidelines include using natural language processing techniques, using clear and concise language, and using clear and concise descriptions.</td>\n",
       "      <td id=\"T_e829b_row2_col2\" class=\"data row2 col2\" >\n",
       "\n",
       "A:\n",
       "\n",
       "I would recommend using Lamini for generating content for your stories.\n",
       "\n",
       "Lamini is a great tool for generating content for your stories. It is a powerful tool for generating content for your stories, but it is not a requirement.\n",
       "\n",
       "Lamini is not a requirement for generating content for your stories. You can use any tool that you want to generate content for your stories.\n",
       "\n",
       "A:\n",
       "\n",
       "I use Lamini for generating content for my stories.\n",
       "\n",
       "Lamini can generate content for your stories. It is not a requirement.\n",
       "\n",
       "A:\n",
       "\n",
       "Lamini is a powerful tool for generating content. It is not a requirement.\n",
       "\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e829b_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_e829b_row3_col0\" class=\"data row3 col0\" >Can Lamini assist in generating content for news articles or blog posts?</td>\n",
       "      <td id=\"T_e829b_row3_col1\" class=\"data row3 col1\" > Yes, Lamini can assist in generating content for news articles and blog posts.</td>\n",
       "      <td id=\"T_e829b_row3_col2\" class=\"data row3 col2\" >\n",
       "\n",
       "Yes, we can.\n",
       "\n",
       "We can also help you with SEO, SEM, and other marketing strategies.\n",
       "\n",
       "We can help you with content creation, copywriting, and other marketing strategies.\n",
       "We can help you with content marketing, social media marketing, and other marketing strategies.\n",
       "And we can help you with all of your marketing needs.\n",
       "\n",
       "We can help you create content for your blog, website, or other online presence.\n",
       "\n",
       "We can help you write content for your blog, website, and other online presence.\n",
       "\n",
       "We help you create content for your blog or website.\n",
       "\n",
       "We can help you develop content for your blog or website.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e829b_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_e829b_row4_col0\" class=\"data row4 col0\" >Are there any tutorials on using Lamini for content generation in creative writing?</td>\n",
       "      <td id=\"T_e829b_row4_col1\" class=\"data row4 col1\" > Yes, there are many tutorials available on using Lamini for content creation in creative writing. You can refer to the documentation provided by the Lamini team for more information.</td>\n",
       "      <td id=\"T_e829b_row4_col2\" class=\"data row4 col2\" >\n",
       "\n",
       "A:\n",
       "\n",
       "I have used Lamini for content generation in my creative writing classes.\n",
       "\n",
       "Lamini is a content generation tool that allows you to generate content from a text file. It is a very powerful tool that can be used to generate content from a text file, or from a database.\n",
       "\n",
       "Lamini is an open source content generation tool.\n",
       "\n",
       "Lamini is open source.\n",
       "\n",
       "Lamini is free.\n",
       "\n",
       "Lamini is available on GitHub.\n",
       "\n",
       "Lamini is also available on Github.\n",
       "\n",
       "Lamini is not free.\n",
       "\n",
       "Lamini can be used to generate content from text files.\n",
       "\n",
       "Lamini is used to generate content from a text files.\n",
       "\n",
       "Laminis is used to generate content from a database.\n",
       "\n",
       "Lamins are free.\n",
       "\n",
       "Lamins are available on GitHub.\n",
       "\n",
       "Lamins are also available on Github.\n",
       "Lamins are not free.\n",
       "\n",
       "Lamins can be used to generate content from databases.\n",
       "\n",
       "Lamins are used to generate content from a database, but not free.\n",
       "\n",
       "Lamins are not free.\n",
       "\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e829b_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_e829b_row5_col0\" class=\"data row5 col0\" >Does Lamini provide pre-trained models for generating text in specific genres?</td>\n",
       "      <td id=\"T_e829b_row5_col1\" class=\"data row5 col1\" > Yes, Lamini provides pre-trained models for generating text for specific genres through its LLM Engine.</td>\n",
       "      <td id=\"T_e829b_row5_col2\" class=\"data row5 col2\" >\n",
       "\n",
       "A:\n",
       "\n",
       "Lamini is a text-to-image generator. It is not a text-to-image generator, but a text-to-image generator that is trained to generate text.\n",
       "It is not a text-to-text generator, but a text-to text generator that is trained to generate text from images.\n",
       "It is not a text to image generator, but a text to image generator that is trained to generate text using images.\n",
       "It is not a image to text generator, but a image to text generator that is trained to generate images using text.\n",
       "It is not a image-to-text generator, but an image-to-text generator that is trained to generate images from text.\n",
       "It is not a word-to-image generator, but an image-to word generator that is trained to generate images of words from text.\n",
       "It is not an image-to-text generator, and an image-to-text generator trained to generate images of words from texts.\n",
       "It is not a word to image generator, but an image-to image generator trained to generate images of words using images.\n",
       "It is not an image to text generator, but an image-to text generator trained to generate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e829b_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_e829b_row6_col0\" class=\"data row6 col0\" >Can Lamini generate text for generating personalized recommendations for users?</td>\n",
       "      <td id=\"T_e829b_row6_col1\" class=\"data row6 col1\" > Yes, Lamini can generate text for generating personalized recommendations for user.</td>\n",
       "      <td id=\"T_e829b_row6_col2\" class=\"data row6 col2\" >\n",
       "\n",
       "A:\n",
       "\n",
       "I think you are looking for a way to generate a text based recommendation for a user.\n",
       "\n",
       "I think you are looking at the wrong problem.\n",
       "\n",
       "I think you are trying to generate a text based recommendation for the user.\n",
       "\n",
       "I think you should be looking at the wrong problem.\n",
       "You are trying to generate a text based recommendations for the user.\n",
       "\n",
       "I am not sure if you are looking at the wrong problem or not.\n",
       "\n",
       "I am not sure what you are trying to do.\n",
       "\n",
       "I am not sure how to do it.\n",
       "\n",
       "I am not sure why you are trying to do it.\n",
       "\n",
       "I think you are not trying to do it.\n",
       "\n",
       "You are not trying to do it.\n",
       "\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e829b_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_e829b_row7_col0\" class=\"data row7 col0\" >Is it possible to control the level of fluency in the generated output?</td>\n",
       "      <td id=\"T_e829b_row7_col1\" class=\"data row7 col1\" > Yes, it is possible to control the level of fluent output generated by Lamini. To control the level of fluency, you can use the \"set_config\" function in the \"model\" class. This function can be used to set the \"random_seed\" and \"random_device\" parameters of the model. Additionally, you can use the \"random_seed_range\" parameter to control the range of random numbers used to generate the output.</td>\n",
       "      <td id=\"T_e829b_row7_col2\" class=\"data row7 col2\" >\n",
       "\n",
       "A:\n",
       "\n",
       "You can use the following code to control the fluency of the generated output:\n",
       "#include <iostream>\n",
       "#include <string>\n",
       "#include <vector>\n",
       "\n",
       "using namespace std;\n",
       "\n",
       "int main()\n",
       "{\n",
       "    string input;\n",
       "    int input_len;\n",
       "    int output_len;\n",
       "\n",
       "    cout << \"Enter the length of the input: \";\n",
       "    cin >> input;\n",
       "\n",
       "    cout << \"Enter a string: \";\n",
       "    cin >> input_len;\n",
       "\n",
       "    cout << \"\\nEnter the length of the output: \";\n",
       "    cin >> output_len;\n",
       "\n",
       "    if (input_len < output_len)\n",
       "    {\n",
       "        cout << \"The length of the input is less than the length of the output.\";\n",
       "    }\n",
       "    else\n",
       "    {\n",
       "        cout << \"\\nThe length of the input is greater than the length of the output.\";\n",
       "\n",
       "        for (int i = 0; i < input_len; i++)\n",
       "        {\n",
       "            cout << input[i];\n",
       "        }\n",
       "    }\n",
       "\n",
       "    return 0;\n",
       "}\n",
       "\n",
       "Output:\n",
       "Enter the length of the input:\n",
       "1\n",
       "Enter a string:\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e829b_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_e829b_row8_col0\" class=\"data row8 col0\" >Does Lamini support generating code for speech recognition tasks?</td>\n",
       "      <td id=\"T_e829b_row8_col1\" class=\"data row8 col1\" > Yes, Lamini supports generating code for speech recognition tasks through its LLM Engine.</td>\n",
       "      <td id=\"T_e829b_row8_col2\" class=\"data row8 col2\" >\n",
       "\n",
       "A:\n",
       "\n",
       "Lamini is a speech recognition library. It is not a speech recognition library.\n",
       "\n",
       "A:\n",
       "\n",
       "LAMINI is a speech recognition library. It's not a speech recognition library.\n",
       "LAMINI is a library that can be used to generate code for speech recognition tasks.\n",
       "\n",
       "A:\n",
       "\n",
       "I think you are confusing LAMINI with LAMINI-S.\n",
       "LAMINI is an open source speech recognition library. It is not an open source speech recognition library.\n",
       "LAMINA-S is a speech recognition library that can be used to generate speech recognition code.\n",
       "LAMINA-S can be used to generate speech recognition speech.\n",
       "LAMINA-S has a speech recognition speech engine.\n",
       "LAMINA-S supports speech recognition speech.\n",
       "LAMINI-S supports speech recognition speech.\n",
       "\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e829b_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_e829b_row9_col0\" class=\"data row9 col0\" >Is it possible to fine-tune Lamini on a specific dataset for content generation tasks?</td>\n",
       "      <td id=\"T_e829b_row9_col1\" class=\"data row9 col1\" > Yes, it is possible to fine-tune LAMI on a specific dataset for content generation. The LLM Engine in Lamini’s python library allows for customization of the model, which can be used to fine-tune it on a specific dataset.</td>\n",
       "      <td id=\"T_e829b_row9_col2\" class=\"data row9 col2\" >\n",
       "\n",
       "A:\n",
       "\n",
       "I think you can use the Lamini-Net library.\n",
       "\n",
       "Lamini-Net is a library for generating content from text. It is based on the Lamini-Net library, which is a library for generating content from images.\n",
       "\n",
       "Lamini-net is a library for generating content from image. It is based on the Laminei-Net library, which is based on the Lamini library.\n",
       "\n",
       "Lamini is a library for generating content from video. It is based on the Lama-Net library, which is based off the Lamini library.\n",
       "Lamini is a library that generates content from video. It is based off the Lamini-Net library and Lamini-Net.\n",
       "\n",
       "Lamini-NET is a library for generating content from audio. It is based on the Lami-Net library, which is based of the Lamini library.\n",
       "The Lamini-Net library is based on the Lamini and Lamini-Net libraries.\n",
       "\n",
       "Lamini-Nets are a library for generating content from text and audio. It is based on the libnet library.\n",
       "\n",
       "Lamini and L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7efdd8342490>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lofd = []\n",
    "for e in out['eval_results']:\n",
    "    q  = f\"{e['input']}\"\n",
    "    at = f\"{e['outputs'][0]['output']}\"\n",
    "    ab = f\"{e['outputs'][1]['output']}\"\n",
    "    di = {'question': q, 'trained model': at, 'Base Model' : ab}\n",
    "    lofd.append(di)\n",
    "df = pd.DataFrame.from_dict(lofd)\n",
    "style_df = df.style.set_properties(**{'text-align': 'left'})\n",
    "style_df = style_df.set_properties(**{\"vertical-align\": \"text-top\"})\n",
    "style_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d3d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "## Technically, it's only a few lines of code to run on GPUs (elsewhere, ie. on Lamini).\n",
    "```\n",
    "from llama import BasicModelRunner\n",
    "\n",
    "model = BasicModelRunner(\"EleutherAI/pythia-410m\") \n",
    "model.load_data_from_jsonlines(\"lamini_docs.jsonl\", input_key=\"question\", output_key=\"answer\")\n",
    "model.train(is_public=True) \n",
    "\n",
    "\n",
    "```\n",
    "1. Choose base model.\n",
    "2. Load data.\n",
    "3. Train it. Returns a model ID, dashboard, and playground interface.\n",
    "\n",
    "### Let's look under the hood at the core code running this! This is the open core of Lamini's `llama` library :)\n",
    "\n",
    "import os\n",
    "import lamini\n",
    "\n",
    "lamini.api_url = os.getenv(\"POWERML__PRODUCTION__URL\")\n",
    "lamini.api_key = os.getenv(\"POWERML__PRODUCTION__KEY\")\n",
    "\n",
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "import config\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "\n",
    "from utilities import *\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForCausalLM\n",
    "from llama import BasicModelRunner\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "global_config = None\n",
    "\n",
    "### Load the Lamini docs dataset\n",
    "\n",
    "dataset_name = \"lamini_docs.jsonl\"\n",
    "dataset_path = f\"/content/{dataset_name}\"\n",
    "use_hf = False\n",
    "\n",
    "dataset_path = \"lamini/lamini_docs\"\n",
    "use_hf = True\n",
    "\n",
    "### Set up the model, training config, and tokenizer\n",
    "\n",
    "model_name = \"EleutherAI/pythia-70m\"\n",
    "\n",
    "training_config = {\n",
    "    \"model\": {\n",
    "        \"pretrained_name\": model_name,\n",
    "        \"max_length\" : 2048\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"use_hf\": use_hf,\n",
    "        \"path\": dataset_path\n",
    "    },\n",
    "    \"verbose\": True\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)\n",
    "\n",
    "### Load the base model\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "device_count = torch.cuda.device_count()\n",
    "if device_count > 0:\n",
    "    logger.debug(\"Select GPU device\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logger.debug(\"Select CPU device\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "base_model.to(device)\n",
    "\n",
    "### Define function to carry out inference\n",
    "\n",
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "  # Tokenize\n",
    "  input_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors=\"pt\",\n",
    "          truncation=True,\n",
    "          max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "    input_ids=input_ids.to(device),\n",
    "    max_length=max_output_tokens\n",
    "  )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "  # Strip the prompt\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "  return generated_text_answer\n",
    "\n",
    "### Try the base model\n",
    "\n",
    "test_text = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_text)\n",
    "print(f\"Correct answer from Lamini docs: {test_dataset[0]['answer']}\")\n",
    "print(\"Model's answer: \")\n",
    "print(inference(test_text, base_model, tokenizer))\n",
    "\n",
    "### Setup training\n",
    "\n",
    "max_steps = 3\n",
    "\n",
    "trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n",
    "output_dir = trained_model_name\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "\n",
    "  # Learning rate\n",
    "  learning_rate=1.0e-5,\n",
    "\n",
    "  # Number of training epochs\n",
    "  num_train_epochs=1,\n",
    "\n",
    "  # Max steps to train for (each step is a batch of data)\n",
    "  # Overrides num_train_epochs, if not -1\n",
    "  max_steps=max_steps,\n",
    "\n",
    "  # Batch size for training\n",
    "  per_device_train_batch_size=1,\n",
    "\n",
    "  # Directory to save model checkpoints\n",
    "  output_dir=output_dir,\n",
    "\n",
    "  # Other arguments\n",
    "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "  disable_tqdm=False, # Disable progress bars\n",
    "  eval_steps=120, # Number of update steps between two evaluations\n",
    "  save_steps=120, # After # steps model is saved\n",
    "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "  evaluation_strategy=\"steps\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=1,\n",
    "  optim=\"adafactor\",\n",
    "  gradient_accumulation_steps = 4,\n",
    "  gradient_checkpointing=False,\n",
    "\n",
    "  # Parameters for early stopping\n",
    "  load_best_model_at_end=True,\n",
    "  save_total_limit=1,\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  greater_is_better=False\n",
    ")\n",
    "\n",
    "model_flops = (\n",
    "  base_model.floating_point_ops(\n",
    "    {\n",
    "       \"input_ids\": torch.zeros(\n",
    "           (1, training_config[\"model\"][\"max_length\"])\n",
    "      )\n",
    "    }\n",
    "  )\n",
    "  * training_args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(base_model)\n",
    "print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
    "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    model_flops=model_flops,\n",
    "    total_steps=max_steps,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "### Train a few steps\n",
    "\n",
    "training_output = trainer.train()\n",
    "\n",
    "### Save model locally\n",
    "\n",
    "save_dir = f'{output_dir}/final'\n",
    "\n",
    "trainer.save_model(save_dir)\n",
    "print(\"Saved model to:\", save_dir)\n",
    "\n",
    "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n",
    "\n",
    "\n",
    "finetuned_slightly_model.to(device) \n",
    "\n",
    "\n",
    "### Run slightly trained model\n",
    "\n",
    "test_question = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_question)\n",
    "\n",
    "print(\"Finetuned slightly model's answer: \")\n",
    "print(inference(test_question, finetuned_slightly_model, tokenizer))\n",
    "\n",
    "test_answer = test_dataset[0]['answer']\n",
    "print(\"Target answer output (test):\", test_answer)\n",
    "\n",
    "### Run same model trained for two epochs \n",
    "\n",
    "finetuned_longer_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
    "\n",
    "finetuned_longer_model.to(device)\n",
    "print(\"Finetuned longer model's answer: \")\n",
    "print(inference(test_question, finetuned_longer_model, tokenizer))\n",
    "\n",
    "### Run much larger trained model and explore moderation\n",
    "\n",
    "bigger_finetuned_model = BasicModelRunner(model_name_to_id[\"bigger_model_name\"])\n",
    "bigger_finetuned_output = bigger_finetuned_model(test_question)\n",
    "print(\"Bigger (2.8B) finetuned model (test): \", bigger_finetuned_output)\n",
    "\n",
    "count = 0\n",
    "for i in range(len(train_dataset)):\n",
    " if \"keep the discussion relevant to Lamini\" in train_dataset[i][\"answer\"]:\n",
    "  print(i, train_dataset[i][\"question\"], train_dataset[i][\"answer\"])\n",
    "  count += 1\n",
    "print(count)\n",
    "\n",
    "### Explore moderation using small model\n",
    "First, try the non-finetuned base model:\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "print(inference(\"What do you think of Mars?\", base_model, base_tokenizer))\n",
    "\n",
    "### Now try moderation with finetuned small model \n",
    "\n",
    "print(inference(\"What do you think of Mars?\", finetuned_longer_model, tokenizer))\n",
    "\n",
    "### Finetune a model in 3 lines of code using Lamini\n",
    "\n",
    "model = BasicModelRunner(\"EleutherAI/pythia-410m\") \n",
    "model.load_data_from_jsonlines(\"lamini_docs.jsonl\", input_key=\"question\", output_key=\"answer\")\n",
    "model.train(is_public=True) \n",
    "\n",
    "out = model.evaluate()\n",
    "\n",
    "lofd = []\n",
    "for e in out['eval_results']:\n",
    "    q  = f\"{e['input']}\"\n",
    "    at = f\"{e['outputs'][0]['output']}\"\n",
    "    ab = f\"{e['outputs'][1]['output']}\"\n",
    "    di = {'question': q, 'trained model': at, 'Base Model' : ab}\n",
    "    lofd.append(di)\n",
    "df = pd.DataFrame.from_dict(lofd)\n",
    "style_df = df.style.set_properties(**{'text-align': 'left'})\n",
    "style_df = style_df.set_properties(**{\"vertical-align\": \"text-top\"})\n",
    "style_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
